
8주차_주간 메모(AWS)

2023.10.10 (화)
개요- Mariadb(mysql) 데이터베이스를 아마존에서 다루어본다.

#1. RDS (Amazon RDS; Amazon Relational Database Service(RDS))
-실습 목차
MySQL 데이터베이스를 실행할 환경을 만들기
데이터베이스에 연결하기
데이터베이스 인스턴스의 삭제    
-실습 환경
region : AP-Northeast-2(서울)
VPC: default
AZ: ap-northeast-2a
	ap-northeast-2c
Resource: Amazon EC2 , Amazon RDS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1. 마리아 데이터베이스 (maria db) 인스턴스 생성
db.t2.micro DB instance class, 20GB, 보존기간 1일(24h), auto Backup을 갖춘 MySQL DB인스턴스를 생성하도록 한다. 
>데이터베이스 생성 클릭
표준생성> 엔진옵션을 선택해야하는데, Aurora와 MySQL와 같은 오픈소스 엔진이 있다.
*Aurora는 프리티어가 아님. !
mariadb를 선택하고  엔진버전은 10.11.5 로 선택
>실습이기때문에 템플릿_ FREE TIER 을 선택한다.
>설정
DB인스턴스 식별자 이름을 자유롭게 설정한다.
' my-rds-mariadb-1 ' 로 정해본다
자격증명_ master user이름을 admin으로 설정하고 
실습이므로 암호자동생성이 아니라 마스터 암호를 'password'로 설정
>인스턴스 구성 
인스턴스 클래스- 버스터블 클래스(t클래스)_ db.t3.micro를 선택.
>스토리지 
스토리지 유형, 할당된 스토리지가 있고 +'스토리지 자동조정(auto scailing)이 있다. 디폴트가 켜져있고 실습이니 자동조정기능은 제외해본다.
>연결
기본 설정을 따르며, 퍼블릭 액세스를 '예'로 선택
보안그룹 새로 생성- mariadb-sg
가용영역- ap-northeast-2a
데이터베이스 포트: 3306인것을 기억하자.
>이후 생성한다. 인스턴스 사용가능으로 바뀌면 제반사항을 확인하도록 한다.

2.  mysql workbench를 설치한다. MariaDB 엔진옵션으로 생성.
>workbench를 구글링한다. https://dev.mysql.com/downloads/workbench/
.msi 파일을 다운로드 받는다 (windows 버전용) 
>아마존 RDS 인스턴스 엔드포인트를 복사해둔다
my-rds-mariadb-1.cgvwqikhep67.ap-northeast-2.rds.amazonaws.com
>connection 에 들어가서 
호스트네임에 엔드포인트를 입력 , 유저네임을 admin, store in vault 에 password 입력
>이러면 새로운 MySQL 이 생성된다.
 
> 더블클릭으로 들어간다. 
데이터베이스 스키마(schema)를 만들 수 있는 관리도구들이 나타난다. 
new schema 1를 만들어본다. 
 
이를 apply로 누르면 output에 로그가 뜨면서 적용된것을 확인 가능. 
>삭제는 RDS에서 삭제하면 된다. 

3. MySQL 엔진옵션으로 생성
프리티어> database-1 >
>자격증명설정
admin / password 설정한다.
버스터블 클래스 db.t3.micro 설정
퍼블릭액세스 '예', 방화벽은 아까만든것처럼 mariadb-sg 선택하고 DB를 생성. 
2에서 했던 과정처럼 똑같이 실습하면 된다.


(3번실습 생성중에 할 것)
4. Aws DynamoDB 구현(실습)  > '데이터베이스'>'DynamoDB'
 다이나모 데이터베이스는 NoSQL 테이블을 만들고, NoSQL 테이블에 데이터를 추가할 수 있다.
>실습으로 테이블 이름을 'Music-table'으로 설정할 것임.
  파티션 키를  Artist 입력
  정렬 키를 SongTitle 입력
테이블 생성.
>NoSQL에 Data를 추가한다. (테이블>항목탐색> 항목생성)
Artist - 'ZICO' / binzeeno / coldplay
SongTitle '아무노래' / aqua man / universe  등등  생성.
>항목 스캔 또는 쿼리
쿼리에서 파티션에 해당하는 값을 입력하면 해당 아티스트나 타이틀이 등장.
(이 단계에서는 따로 설정을 안하면 대소문자 구분을 하지 못한다.)
*대소문자 구분방식(리눅스)
 show variables like 'lower_case_table_names';
lower_case_table_names의 값이 0인지 1인지 2인지를 확인해야 한다.

0일 경우 : 대소문자 구분함
1일 경우 : 대소문자 구분안함		이므로 구분안함을 위해서 값을 1로 설정해준다.
MySQL의 my.cnf 파일을 건드려 주어야 하는데 보통은 /etc/my.cnf에 존재한다.
vi로 my.cnf 파일을 보도록 하자
영역의 맨밑에 
lower_case_table_names = 1
라고 설정을 해주도록 하자. 그리고 mysqld 서비스를 재시작 한 뒤 쿼리에서 명령어 실행하여 대소문자 구분값이 변경되었는지 확인하자.

5. AWS Aurora I/O 이용하기
>aurora   db를 선택한다.
*탬플릿에 프리티어는 없다
*시험대비문제:
응용 프로그램 지연 문제를 완화시키는 대체 아키텍처를 권장해야 합니다. 대체 아키텍처는 또한 개발팀이 지연 없이 스테이징 환경을 계속 사용할 수 있어야 합니다.어떤 솔루션이 이러한 요구 사항을 충족합니까?
답안: RDS에 비해 Aurora는 항상 3개의 AZ에 6개의 복제본(Replica)를 보유하고 있으므로 애플리케이션 가용성에 더 유리하다.

어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?
답: 데이터베이스를 Amazon Aurora MySQL DB 인스턴스로 마이그레이션합니다. 웹 애플리케이션의 AMI를 생성합니다. 시작 템플릿에 AMI를 적용합니다. 시작 템플릿으로 Auto Scaling 그룹을 생성합니다. 스팟 집합을 사용하도록 시작 템플릿을 구성합니다. Auto Scaling 그룹에 Application Load Balancer를 연결합니다.
-Aurora MySQL DB 인스턴스로 마이그레이션: DB 조정할 필요 없이 자동으로 크기 조정
-시작 템플릿에 AMI 적용: 향후 인스턴스를 원할하게 생성할 수 있음
-Auto Scaling 그룹에 추가: 수요에 따라 확장 및 축소되므로 비용 절약 가능
-스팟 집합: 인스턴스를 중지해서는 안된다는 말이 나온 것도 아니고, Auto Scaling이 명시되어 있으므로 가장 비용 효율적인 솔루션임

오답_:자동 백업에 대한 수명 주기 정책을 만듭니다. / 5년 동안 자동 백업 보존을 구성합니다.
Aurora의 자동 백업의 최대 보존 기간은 35일

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#2. AWS Lambda (serverless)
아마존의 람다는 서버를 프로비저닝, 직접 관리하지 않고도 코드를 실행할 수 있게하는 컴퓨팅 서비스이다. 여기서 프로비저닝이란, 통상적으로 Infra(자동화, 배포 등) 를 준비하기 위한 전체적인 절차 구상을 생각하면 된다.   자바스크립트에서 보통 많이 쓰임.

>함수생성
람다>함수>함수생성으로 들어간다. (블루프린트 탬플릿을 사용)
새로작성- Hello_lambda / 런타임을 python.선택/ 아키텍처는 x86
aws정책 탬플릿에서 새 역할 생성_ my-lambda-ex  /  람다함수 생성하기.
>테스트(이벤트)
hello_lambda, 코드 소스 이벤트를 생성하고 >test 'Execution results'탭에 결과가 뜬다
응답이 Response에 출력됨. 또한 펑션로그가 남게된다.

***시험준비: 회사는 데이터베이스에 로드해야 하는 대용량 데이터를 처리하기 위해 Lambda 할당량을 크게 늘려야 합니다. 솔루션 설계자는 확장성을 개선하고 구성 노력을 최소화하기 위해 새로운 설계를 권장해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
-답: 두개의 람다함수 설정. 정보를 수신할 하나의 기능을 구성하십시오. 정보를 데이터베이스에 로드하도록 다른 기능을 구성하십시오. Amazon Simple Queue Service(Amazon SQS) 대기열을 사용하여 Lambda 함수를 통합합니다.
-대용량 데이터 처리 + 확장성 개선 = SQS + Lambda

>s3 버킷만들어 연동
bk-###-gmail-com
객체소유권을 acl 활성화됨
버킷을 생성했음
>IAM 정책생성으로 넘어감
 정책생성>권한지정 json 모드>
s3-trigger-tutorial
>IAM 역할로 넘어감
aws서비스-lambda 선택 >s3-trigger-tutorial 선택후 역할생성.

>람다로 넘어가서
함수생성, 기존 역할 사용> 람다 s3 트리거 롤을 선택 후 함수 생성.

>함수코드의 배포(파이썬을 이용해봄)
코드에 들어가서 
import json
import urllib.parse
import boto3
print('Loading function')
s3 = boto3.client('s3')
def lambda_handler(event, context):
    #print("Received event: " + json.dumps(event, indent=2))
    # Get the object from the event and show its content type
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        print("CONTENT TYPE: " + response['ContentType'])
        return response['ContentType']
    except Exception as e:
        print(e)
        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))
        raise e
라는 코드를 저장하고 배포.(deploy)              

>트리거 추가에서 s3 트리거 설정.
내가만든 버킷 설정, 이벤트 유형에서 모든객체 생성이벤트 선택(되어있음)
>더미이벤트로 함수테스트.
탬플릿을 s3-put으로 설정, event json에서 값을       
6라인 "awsRegion": "ap-northeast-2",
로 변경하고 example-bucket의 인스턴스 이름을 변경
23, 27라인           "name": "bk-###-gmail-com",
>test%2FKey를 이전에 업로드한 테스트 객체의 이름으로 변경.
30라인            "key": "coldplay_viva_la_vida_lyrics.txt",
이렇게 수정한 뒤 저장.
>>안된다. '함수 코드 배포' 단계에서 런타임 신택스오류가 난다.
https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/with-s3-example.html#with-s3-example-create-function 참조.
>>런타임을 16, 14로 내렸다. 그러니까 된다.
Response
"text/plain"

Function Logs
START RequestId: 377740e9-139f-47e8-b5e9-a57c8eb6dd96 Version: $LATEST
2023-10-10T07:53:38.471Z	377740e9-139f-47e8-b5e9-a57c8eb6dd96	INFO	CONTENT TYPE: text/plain
END RequestId: 377740e9-139f-47e8-b5e9-a57c8eb6dd96
REPORT RequestId: 377740e9-139f-47e8-b5e9-a57c8eb6dd96	Duration: 677.94 ms	Billed Duration: 678 ms	Memory Size: 128 MB	Max Memory Used: 80 MB	Init Duration: 440.99 ms

Request ID
377740e9-139f-47e8-b5e9-a57c8eb6dd96
이렇게 뜬다.
>>>느낀것: 신택스오류가 나는것은 버전호환이 안되는 경우도 종종있으니까 버전을 낮춰서 호환성을 높여볼것.!

>Amazon S3 트리거를 사용하여 썸네일 이미지 생성(?) 미완
 
-원본 및 대상 Amazon S3 버킷을 생성하고 샘플 이미지를 업로드합니다.
-이미지 크기를 조정하고 Amazon S3 버킷에 썸네일을 출력하는 Lambda 함수를 생성합니다.
-객체가 원본 버킷에 업로드될 때 함수를 간접적으로 호출하는 Lambda 트리거를 구성합니다.
-먼저 더미 이벤트를 사용한 다음 원본 버킷에 이미지를 업로드하여 함수를 테스트하세요.

>먼저 sourcebk-1 , 2 두개를 만든다.
소스1 버킷에 이미지파일을 업로드했다.
>권한정책을 생성한다.
IAM>정책>정책생성>json편집기에 아래내용을 붙여넣는다. "cloudwatch,s3내용"+리소스는 log,s3인듯함 
 
LambdaS3Policy 라고 선택.
>역할생성 , 신뢰할수있는 엔터티를 aws서비스, 사례 lambda
LambdaS3Role 이름으로 역할만들기.
>람다 3role로 함수를 만들어두기.
>일단정지했다.

##AWS자습
>피크시간에 수십만 명에게 서비스 제공& 확장가능한 실시간 솔루션, 민감한 데이터 제거용 트랜잭션(거래내역) 처리는 Amazon Kinesis Data Streams을 사용한다.
-피크 시간에 수십만 명의 사용자에게 서비스 제공 = Kinesis
-데이터 수집을 위해서 Kinesis Data Streams 사용 / Kinesis Data Firehose는 데이터 변환 및 전송 서비스

>AWS Config를 사용하여 구성 변경을 추적하고 AWS CloudTrail을 사용하여 API 호출을 기록합니다.
AWS Config = 리소스 구성 사항 변경 추적
AWS CloudTrail = 리소스 내역 기록

>(ELB) 뒤의 VPC 내 Amazon EC2 인스턴스로 구성됩니다. DNS에는 타사 서비스가 사용됩니다. 회사의 솔루션 설계자는 대규모 DDoS 공격을 감지하고 보호할 솔루션을 권장해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
(O) AWS Shield Advanced를 활성화하고 ELB를 할당합니다
(X) AWS Shield를 활성화하고 여기에 Amazon Route 53을 할당합니다.
대규모 DDoS 방어는 AWS Shield Advanced가 더 적합

>회사는 인스턴스에 원격으로 안전하게 액세스하고 관리하는 전략을 수립해야 합니다. 회사는 기본 AWS 서비스와 함께 작동하고 AWS Well-Architected 프레임워크를 따르는 반복 가능한 프로세스를 구현해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
-각 기존 인스턴스와 새 인스턴스에 적절한 IAM 역할을 연결합니다. AWS Systems Manager Session Manager를 사용하여 원격 SSH 세션을 설정합니다.

>회사 웹 사이트를 통해 이 데이터에 대한 수백만 건의 업데이트가 매일 있습니다. 회사는 일부 작업이 10초 이상 걸리는 것을 발견하고 데이터베이스 스토리지 성능이 병목 현상이라고 판단했습니다. 어떤 솔루션이 성능 문제를 해결합니까?
(O) 스토리지 유형을 프로비저닝된 IOPS SSD로 변경합니다.
업데이트 = I/O 성능 관련
스토리지 성능의 문제 = 스토리지 유형 변경
범용 SSD 스토리지가 있다고 문제에서 언급함
프로비저닝된 IOPS 볼륨은 SSD로 지원되며 중요한 I/O 집악적인 데이터베이스 애플리케이션을 위해 설계된 최고 성능의 EBS 볼륨이다.

>사용자는 통화 후 1년 이내에 파일에 무작위로 액세스하지만 1년 이후에는 파일에 자주 액세스하지 않습니다. 이 회사는 사용자에게 1년 미만의 파일을 가능한 한 빨리 쿼리하고 검색할 수 있는 기능을 제공하여 솔루션을 최적화하려고 합니다. 오래된 파일을 검색하는 데 있어 지연은 허용됩니다. 어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?
(O) Amazon S3 Intelligent-Tiering에 개별 파일을 저장합니다. S3 수명 주기 정책을 사용하여 1년 후 파일을 S3 Glacier Flexible Retrieval로 이동합니다. Amazon Athena를 사용하여 Amazon S3에 있는 파일을 쿼리하고 검색합니다. S3 Glacier Select를 사용하여 S3 Glacier에 있는 파일을 쿼리하고 검색합니다.
무작위 액세스 = S3 Intelligent-Tiering
오래된 파일을 검색하는 데 있어 지연 허용 = S3 Glacier Flexible Retrieve
비교: 즉각적인 액세스 필요 = S3 Glacier Instant Retrieve

########################################################################
